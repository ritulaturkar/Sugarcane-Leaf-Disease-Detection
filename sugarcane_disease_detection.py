# -*- coding: utf-8 -*-
"""Sugarcane disease detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iUl0HVpdoV2pTJt7hs-MLI4ZsM08IA9v
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import matplotlib.pyplot as plt

# Data preprocessing and augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    "/content/drive/MyDrive/Sugarcane Leaf Image Dataset",
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'  # Changed to 'categorical'
)

validation_generator = test_datagen.flow_from_directory(
    "/content/drive/MyDrive/Sugarcane Leaf Image Dataset",
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'  # Changed to 'categorical'
)

# Load pre-trained ResNet50 model + higher level layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Unfreeze the last few layers of the base model
for layer in base_model.layers[-10:]:
    layer.trainable = True

# Add custom layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x)  # Add dropout to prevent overfitting
predictions = Dense(3, activation='softmax')(x)  # Changed to 3 units and 'softmax'

model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Generate model summary
model.summary()

# Define callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    epochs=10,  # Increase the number of epochs
    callbacks=[reduce_lr, early_stopping]
)

# Save the model
model.save('/content/drive/MyDrive/sugarcane_leaf_model.h5')

# Load the model (for demonstration purposes, you would typically do this in a separate script)
from tensorflow.keras.models import load_model
loaded_model = load_model('/content/drive/MyDrive/sugarcane_leaf_model.h5')

# Evaluate the loaded model
test_generator = test_datagen.flow_from_directory(
    "/content/drive/MyDrive/Sugarcane Leaf Image Dataset",
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'  # Changed to 'categorical'
)

evaluation = loaded_model.evaluate(test_generator)
print(f"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}")

# Plot the training history
plt.figure(figsize=(12, 4))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.show()

from google.colab import drive
drive.mount('/content/drive')